{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# REQUIRED LIBRARIES\n",
    "# ==============================\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from dipy.io.image import load_nifti\n",
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.reconst.dti import TensorModel\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# DEFINES DATA DIRECTORIES AND CONFIGURATIONS\n",
    "# ==============================\n",
    "DATA_DIR = \"../data/raw\"\n",
    "\n",
    "OUTPUT_DIR = \"../data/features\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SAMPLE_SUBJECT = \"sub-01\" # For debugging\n",
    "\n",
    "# Loads a standardized brain atlas (Harvard-Oxford Atlas),\n",
    "# which divides the brain into predefined regions.\n",
    "atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "masker = NiftiLabelsMasker(labels_img = atlas.maps, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# FUNCTION: EXTRACTS T1W and T2W FEATURES BY REGION\n",
    "# ==============================\n",
    "def extract_t1_t2_features(subject_id):\n",
    "    features = {\"subject\": subject_id}\n",
    "\n",
    "    for scan_type in [\"T1w\", \"T2w\"]: # Loops over both T1-weighted and T2-weighted scans\n",
    "        scan_path = os.path.join(DATA_DIR, subject_id, \"anat\", f\"{subject_id}_{scan_type}.nii.gz\")\n",
    "\n",
    "        if os.path.exists(scan_path):\n",
    "            img = nib.load(scan_path)\n",
    "\n",
    "            # Extracts region-wise mean signal intensity using the Harvard-Oxford Atlas\n",
    "            region_data = masker.fit_transform(img) \n",
    "            \n",
    "            # Stores region-wise mean signal intensity\n",
    "            for i, region_value in enumerate(region_data.flatten()):\n",
    "                features[f\"{scan_type}_region_{i}_mean\"] = np.mean(region_value)\n",
    "\n",
    "        else:\n",
    "            print(f\"Missing {scan_type} for {subject_id}\")\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# SAMPLE FEATURE EXTRACTION - DEBUGGING\n",
    "# ==============================\n",
    "sample_t1_t2_features = extract_t1_t2_features(SAMPLE_SUBJECT)\n",
    "print(sample_t1_t2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# FUNCTION: EXTRACTS DWI FEATURES BY REGION\n",
    "# ==============================\n",
    "def extract_dwi_features(subject_id):\n",
    "    features = {\"subject\": subject_id}\n",
    "\n",
    "    # All three file paths are needed for diffusion-weighted imaging (DWI) data\n",
    "    dwi_path = os.path.join(DATA_DIR, subject_id, \"dwi\", f\"{subject_id}_acq-lr_dwi.nii.gz\")\n",
    "    bval_path = os.path.join(DATA_DIR, subject_id, \"dwi\", f\"{subject_id}_acq-lr_dwi.bval\")\n",
    "    bvec_path = os.path.join(DATA_DIR, subject_id, \"dwi\", f\"{subject_id}_acq-lr_dwi.bvec\")\n",
    "\n",
    "    if os.path.exists(dwi_path):\n",
    "        data, affine = load_nifti(dwi_path)\n",
    "        bvals = np.loadtxt(bval_path) # Loads b-values (gradient strength) as bvals\n",
    "        bvecs = np.loadtxt(bvec_path) # Loads b-vectors (gradient direction) as bvecs\n",
    "        gtab = gradient_table(bvals, bvecs) # A gradient_table helps standardize DWI data across scanner machines\n",
    "\n",
    "        # Fitting the model - needed in order to make fa, md, rd, and ad calculations.\n",
    "        model = TensorModel(gtab)\n",
    "        tensor_fit = model.fit(data)\n",
    "\n",
    "        # Fractional Anisotropy (FA) - Measures how directional water diffusion is\n",
    "        # Higher FA -> More structured white matter\n",
    "        fa_regions = masker.fit_transform(nib.Nifti1Image(tensor_fit.fa, affine)) \n",
    "        # Mean Diffusivity (MD) - Measures overall diffusion in all directions\n",
    "        # Higher MD -> More free water diffusion (vs restricted water diffusion)\n",
    "        md_regions = masker.fit_transform(nib.Nifti1Image(tensor_fit.md, affine)) \n",
    "        # Radial Diffusivity (RD) - Measures diffusion perpendicular to white matter fibers\n",
    "        # Lower RD -> More intact myelin\n",
    "        rd_regions = masker.fit_transform(nib.Nifti1Image(tensor_fit.rd, affine)) \n",
    "        # Axial Diffusivity (AD) - Measures diffusion along the main fiber direction\n",
    "        # Higher AD -> Stronger axonal integrity\n",
    "        ad_regions = masker.fit_transform(nib.Nifti1Image(tensor_fit.ad, affine))\n",
    "\n",
    "        # Stores mean diffusion measures per region\n",
    "        for i in range(fa_regions.shape[1]): \n",
    "            features[f\"dwi_region_{i}_fa_mean\"] = np.mean(fa_regions[:, i])\n",
    "            features[f\"dwi_region_{i}_md_mean\"] = np.mean(md_regions[:, i])\n",
    "            features[f\"dwi_region_{i}_rd_mean\"] = np.mean(rd_regions[:, i])\n",
    "            features[f\"dwi_region_{i}_ad_mean\"] = np.mean(ad_regions[:, i])\n",
    "            \n",
    "    else:\n",
    "        print(f\"Missing DWI files for {subject_id}\")\n",
    "\n",
    "    return features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# SAMPLE FEATURE EXTRACTION - DEBUGGING\n",
    "# ==============================\n",
    "sample_dwi_features = extract_dwi_features(SAMPLE_SUBJECT)\n",
    "print(sample_dwi_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# FUNCTION: EXTRACTS FMRI FEATURES BY REGION\n",
    "# ==============================\n",
    "def extract_fmri_features(subject_id):\n",
    "    features = {\"subject\": subject_id}\n",
    "\n",
    "    fmri_path = os.path.join(DATA_DIR, subject_id, \"func\", f\"{subject_id}_task-rest_run-01_bold.nii.gz\")\n",
    "    \n",
    "    if os.path.exists(fmri_path):\n",
    "        img = nib.load(fmri_path)\n",
    "\n",
    "        # Fits the atlas to the image data\n",
    "        time_series = masker.fit_transform(img)\n",
    "\n",
    "        # Stores statistical metrics for each atlas region\n",
    "        for i in range(time_series.shape[1]):\n",
    "            features[f\"fmri_region_{i}_mean_activation\"] = np.mean(time_series[:, i])\n",
    "            features[f\"fmri_region_{i}_skew_activation\"] = stats.skew(time_series[:, i])\n",
    "            features[f\"fmri_region_{i}_kurtosis_activation\"] = stats.kurtosis(time_series[:, i])\n",
    "\n",
    "        # Creates a connectivity matrix - a matrix where values represent the \n",
    "        # connectedness of two brain regions. \n",
    "        conn_measure = ConnectivityMeasure(kind=\"correlation\")\n",
    "        connectivity_matrix = conn_measure.fit_transform([time_series])[0]\n",
    "\n",
    "        # Stores only the upper triangle of the connectivity matrix\n",
    "        num_regions = connectivity_matrix.shape[0]\n",
    "        for i in range(num_regions):\n",
    "            for j in range(i+1, num_regions):\n",
    "                features[f\"conn_{i}_{j}\"] = connectivity_matrix[i, j]\n",
    "\n",
    "    else:\n",
    "        print(f\"Missing fMRI for {subject_id}\")\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# SAMPLE FEATURE EXTRACTION - DEBUGGING\n",
    "# ==============================\n",
    "sample_fmri_features =  extract_fmri_features(SAMPLE_SUBJECT)\n",
    "print(sample_fmri_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# EXTRACTS ALL FEATURES FROM FIRST 5 SUBJECTS\n",
    "# ==============================\n",
    "subject_list = sorted([sub for sub in os.listdir(DATA_DIR) if sub.startswith(\"sub-\")])[:5]\n",
    "\n",
    "# Extracts features for each subject\n",
    "all_features = []\n",
    "\n",
    "for subject in subject_list:\n",
    "    print(f\"Extracting features for {subject}:\")\n",
    "    features = {}\n",
    "    features.update(extract_t1_t2_features(subject))\n",
    "    print(f\"1/3 completed.\")\n",
    "    features.update(extract_dwi_features(subject))\n",
    "    print(f\"2/3 completed.\")\n",
    "    features.update(extract_fmri_features(subject))\n",
    "    print(f\"3/3 completed.\")\n",
    "    all_features.append(features)\n",
    "    print(f\"Subject {subject} data extraction completed.\")\n",
    "\n",
    "# Saves to CSV\n",
    "df_features = pd.DataFrame(all_features)\n",
    "output_file = os.path.join(OUTPUT_DIR, \"first_5_subjects_features.csv\")\n",
    "df_features.to_csv(output_file, index=False)\n",
    "print(f\"Feature extraction complete. Data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# EXTRACTS ALL FEATURES FROM ALL SUBJECTS\n",
    "# ==============================\n",
    "all_features = []\n",
    "\n",
    "for subject in sorted(os.listdir(DATA_DIR)):\n",
    "    if subject.startswith(\"sub-\"):\n",
    "        print(f\"Extracting features for {subject}:\")\n",
    "        features = {}\n",
    "        features.update(extract_t1_t2_features(subject))\n",
    "        print(f\"1/3 completed.\")\n",
    "        features.update(extract_dwi_features(subject))\n",
    "        print(f\"2/3 completed.\")\n",
    "        features.update(extract_fmri_features(subject))\n",
    "        print(f\"3/3 completed.\")\n",
    "        all_features.append(features)\n",
    "        print(f\"Subject {subject} data extraction completed.\")\n",
    "\n",
    "# Saves to CSV\n",
    "df_features = pd.DataFrame(all_features)\n",
    "output_file = os.path.join(OUTPUT_DIR, \"extracted_features.csv\")\n",
    "df_features.to_csv(output_file, index=False)\n",
    "print(f\"Feature extraction complete. Data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# EXTRACTS SCORE DATA & MERGES WITH BRAIN SCAN FEATURES\n",
    "# ==============================\n",
    "# File paths\n",
    "SCOREDATA_FILE = os.path.join(DATA_DIR, \"scoredata.csv\")\n",
    "FEATURES_FILE = os.path.join(OUTPUT_DIR, \"extracted_features.csv\")\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"data.csv\")\n",
    "\n",
    "# Creates pandas dataframe for participant score data\n",
    "participants_data_path = os.path.join(DATA_DIR, \"participants.tsv\")\n",
    "df_scores = pd.read_csv(participants_data_path, sep=\"\\t\")\n",
    "\n",
    "# Cleans spaces\n",
    "df_scores[\"participant_id\"] = df_scores[\"participant_id\"].str.strip()\n",
    "\n",
    "# Loads extracted_features.csv as a pandas dataframe\n",
    "df_features = pd.read_csv(FEATURES_FILE)\n",
    "\n",
    "# Renames 'subject' to 'participant_id' - needed for datasets to be joined\n",
    "df_features.rename(columns = {\"subject\": \"participant_id\"}, inplace=True)\n",
    "\n",
    "# Merges the two datasets on 'participant_id'\n",
    "df_final = df_scores.merge(df_features, on=\"participant_id\", how=\"inner\")\n",
    "\n",
    "# Saves to CSV\n",
    "df_final.to_csv(OUTPUT_FILE, index = False)\n",
    "print(f\"Merged data saved to: {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
